# ArabicLLM-NeuroScope
ğŸ¯ Multilingual LLM Evaluation Suite for Arabic NLP.

Analyze activation flow, token strength & semantic bias of prompts in Fusha, Dialect, and English.

## ğŸ§  Overview
ArabicLLM-NeuroScope is a lightweight, reproducible framework for analyzing the behavior of Large Language Models (LLMs) when processing prompts in:

ğŸ‡¬ğŸ‡§ English (reference baseline)
ğŸ‡¸ğŸ‡¦ Modern Standard Arabic (Fusha)
ğŸ—£ï¸ Spoken Arabic Dialects (e.g. Saudi dialect)

It visualizes token activations, layer norms, and prompt flow across transformer layers â€“ both neuronally and semantically.
```
## ğŸ“‚ Folder Structure
ArabicLLM-NeuroScope/
â”œâ”€â”€ prompt_analysis/           # Norm activation analysis
â”œâ”€â”€ nlp_analysis/              # Token-level NLP evaluation
â”œâ”€â”€ visual/                    # Plots (2D/3D PNG & HTML)
â”œâ”€â”€ data/                      # Prompt files & JSON outputs
â”œâ”€â”€ diagrams/                  # Mermaid diagrams & flowcharts
â””â”€â”€ README.md
```
## ğŸ” Prompt Activation Analysis

This module measures token firepower across layers using activation norms:  
$\|h_i\| = \sqrt{h_1^2 + h_2^2 + ... + h_d^2}$

ğŸ“Œ Interpretation:  
Higher norms across more layers â†’ deeper processing  
Lower activation â†’ soft filters, poor embedding, or weak tokenization

ğŸ§ª Example prompts:

- English: `"What is the history of artificial intelligence in the Middle East?"`
- Fusha: `"Ù…Ø§ Ù‡Ùˆ ØªØ§Ø±ÙŠØ® Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ Ø§Ù„Ø´Ø±Ù‚ Ø§Ù„Ø£ÙˆØ³Ø·ØŸ"`
- Dialect: `"ÙˆØ´ ØµØ§Ø± Ù…Ø¹ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ø§Ù„Ø®Ù„ÙŠØ¬ØŸ"`

ğŸ“Š Outputs:

- ğŸ“ˆ [`visual/layer_norm_comparison.png`](visual/layer_norm_comparison.png)
- ğŸ”¥ [`visual/firepower_comparison.png`](visual/firepower_comparison.png)
- ğŸ§  Interactive 3D: [`visual/activation_3d_plot.html`](visual/activation_3d_plot.html)

---

## ğŸ§¬ NLP Semantic Evaluation (NER, Morphology, Token Fragility)

Located in [`nlp_analysis/`](nlp_analysis/)

This module explores:

- ğŸ§  NER entity detection (e.g., tech, countries)
- ğŸ§± Morphological evaluation of dialect prompts
- ğŸ§© Token Fragility â€“ how robust is the tokenizer for Arabic?

ğŸ¯ Goal: Reveal gaps in vocabulary coverage, semantic integrity, and LLM robustness in non-English languages.

ğŸ“ Key Files:

- Entry: [`nlp_analysis/nlp_eval.py`](nlp_analysis/nlp_eval.py)
- Prompts: [`nlp_analysis/prompt_ner_fusha.txt`](nlp_analysis/prompt_ner_fusha.txt)
- NER JSON: [`nlp_analysis/ner_result.json`](nlp_analysis/ner_result.json)
- Spider Plot: [`visual/nlp_spider_plot.png`](visual/nlp_spider_plot.png)
- Interactive: [`nlp_analysis/nlp_3d_eval.html`](nlp_analysis/nlp_3d_eval.html)

---

## ğŸ”­ Visualizations

All plots are located in [`visual/`](visual/)

- ğŸ“ˆ [`layer_norm_comparison.png`](visual/layer_norm_comparison.png): Mean norm per layer by language
- ğŸ”¥ [`firepower_comparison.png`](visual/firepower_comparison.png): Sum of token norm activations
- ğŸ§  [`3D_activation_landscape2.png`](visual/3D_activation_landscape2.png): Prompt vs. Layer vs. Norm
- ğŸ•¸ï¸ [`nlp_spider_plot.png`](visual/nlp_spider_plot.png): Semantic/NLP coverage
- ğŸ§© [`mermaid/mermaid_arabic_llm1.png`](mermaid/mermaid_arabic_llm1.png): Flowchart of entire system

---

## ğŸš€ How to Run

```bash
# Step 1: Prompt Activation (via Norms)
python scripts/analyze_prompt_activation.py \
  --model_path ../patched_model_main_bin \
  --prompt "$(cat prompts/prompt_fusha.txt)" \
  --output reports/fusha_report.json

# Step 2: NLP Evaluation
python nlp_analysis/nlp_eval.py \
  nlp_analysis/prompt_ner_fusha.txt \
  nlp_analysis/ner_result.json

# Step 3: Plotting
python scripts/plot_layer_norms_all.py
python scripts/plot_firepower.py
python nlp_analysis/nlp_spider_plot.py
python scripts/plot_3d_layernorm.py
